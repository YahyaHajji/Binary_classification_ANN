{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLHTVskVJKQDDyA+51wIg0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YahyaHajji/Binary_classification_ANN/blob/master/binary_classification_ann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import Required Libraries**"
      ],
      "metadata": {
        "id": "2llAMN3luq1-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XABxDRProPdX"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "import seaborn as sns\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(\"âœ… Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROJECT 1: Simple Synthetic Binary Classification\n",
        "**Step 2: Create Synthetic Dataset**"
      ],
      "metadata": {
        "id": "llJjzQR8uvqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create synthetic binary classification dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate 1000 random samples with 5 features\n",
        "n_samples = 1000\n",
        "n_features = 5\n",
        "\n",
        "X_synthetic = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Create binary labels based on a condition\n",
        "# Rule: If sum of features > 0, label = 1, else label = 0\n",
        "y_synthetic = (X_synthetic.sum(axis=1) > 0).astype(int)\n",
        "\n",
        "print(\"Synthetic Dataset Information:\")\n",
        "print(f\"Total samples: {n_samples}\")\n",
        "print(f\"Features: {n_features}\")\n",
        "print(f\"Feature matrix shape: {X_synthetic.shape}\")\n",
        "print(f\"Labels shape: {y_synthetic.shape}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(f\"Class 0: {(y_synthetic == 0).sum()} samples ({(y_synthetic == 0).sum()/len(y_synthetic)*100:.1f}%)\")\n",
        "print(f\"Class 1: {(y_synthetic == 1).sum()} samples ({(y_synthetic == 1).sum()/len(y_synthetic)*100:.1f}%)\")\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(['Class 0', 'Class 1'],\n",
        "        [(y_synthetic == 0).sum(), (y_synthetic == 1).sum()],\n",
        "        color=['#FF6B6B', '#4ECDC4'], edgecolor='black', linewidth=2)\n",
        "plt.title('Class Distribution', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_synthetic[:, 0], X_synthetic[:, 1],\n",
        "            c=y_synthetic, cmap='coolwarm', alpha=0.6, edgecolor='black', linewidth=0.5)\n",
        "plt.colorbar(label='Class')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Data Visualization (First 2 Features)', fontsize=12, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M_E7QV8_op8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Split and Normalize Synthetic Data**"
      ],
      "metadata": {
        "id": "y_ocDZYwwPDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test sets\n",
        "X_train_syn, X_test_syn, y_train_syn, y_test_syn = train_test_split(\n",
        "    X_synthetic, y_synthetic, test_size=0.2, random_state=42, stratify=y_synthetic\n",
        ")\n",
        "\n",
        "print(\"Data Split:\")\n",
        "print(f\"Training samples: {X_train_syn.shape[0]}\")\n",
        "print(f\"Test samples: {X_test_syn.shape[0]}\")\n",
        "\n",
        "# Standardize features (important for neural networks!)\n",
        "scaler_syn = StandardScaler()\n",
        "X_train_syn_scaled = scaler_syn.fit_transform(X_train_syn)\n",
        "X_test_syn_scaled = scaler_syn.transform(X_test_syn)\n",
        "\n",
        "print(f\"\\nBefore scaling - Mean: {X_train_syn.mean():.4f}, Std: {X_train_syn.std():.4f}\")\n",
        "print(f\"After scaling - Mean: {X_train_syn_scaled.mean():.4f}, Std: {X_train_syn_scaled.std():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPF7FeTMqNVj",
        "outputId": "1bd566a7-63e9-49cf-f2f9-8f1819eaab11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Split:\n",
            "Training samples: 800\n",
            "Test samples: 200\n",
            "\n",
            "Before scaling - Mean: 0.0133, Std: 0.9972\n",
            "After scaling - Mean: 0.0000, Std: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Build Binary Classification Model**"
      ],
      "metadata": {
        "id": "I9n2iXlJxE7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build ANN for binary classification\n",
        "binary_model = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(n_features,)),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Display model architecture\n",
        "print(\"Binary Classification Model Architecture:\")\n",
        "binary_model.summary()\n",
        "\n",
        "print(f\"\\nðŸ“Š Total parameters: {binary_model.count_params():,}\")\n",
        "print(\"\\nðŸ”‘ Key points:\")\n",
        "print(\"   â€¢ Output layer has 1 neuron (binary classification)\")\n",
        "print(\"   â€¢ Sigmoid activation outputs probability [0, 1]\")\n",
        "print(\"   â€¢ Output > 0.5 â†’ Class 1, Output â‰¤ 0.5 â†’ Class 0\")"
      ],
      "metadata": {
        "id": "WNn0aC5GquLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Compile with Binary Crossentropy**"
      ],
      "metadata": {
        "id": "1Br0lK2Uy6W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "binary_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',  # Binary crossentropy for binary classification\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"âœ… Model compiled with:\")\n",
        "print(\"   â€¢ Optimizer: Adam\")\n",
        "print(\"   â€¢ Loss: Binary Crossentropy\")\n",
        "print(\"   â€¢ Metrics: Accuracy\")"
      ],
      "metadata": {
        "id": "BvZPqeAaq1up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Train the Model**"
      ],
      "metadata": {
        "id": "gBnh6aqmzAP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "print(\"ðŸš€ Training Binary Classification Model...\")\n",
        "\n",
        "history_syn = binary_model.fit(\n",
        "    X_train_syn_scaled, y_train_syn,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Training completed!\")"
      ],
      "metadata": {
        "id": "otceJQr7q5u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Evaluate Performance**"
      ],
      "metadata": {
        "id": "zzNtB1qezOwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = binary_model.evaluate(X_test_syn_scaled, y_test_syn, verbose=0)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š SYNTHETIC DATA - MODEL EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Make predictions\n",
        "predictions = binary_model.predict(X_test_syn_scaled)\n",
        "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "print(f\"\\nSample predictions (first 10):\")\n",
        "print(\"Probability | Predicted | Actual\")\n",
        "print(\"-\" * 40)\n",
        "for i in range(10):\n",
        "    print(f\"{predictions[i][0]:.4f}      | {predicted_classes[i]}         | {y_test_syn[i]}\")"
      ],
      "metadata": {
        "id": "n2UWTIklq-VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Visualize Training History**"
      ],
      "metadata": {
        "id": "nwAmWV-1zm2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "ax1.plot(history_syn.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "ax1.plot(history_syn.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "ax1.set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss plot\n",
        "ax2.plot(history_syn.history['loss'], label='Training Loss', linewidth=2)\n",
        "ax2.plot(history_syn.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "ax2.set_title('Model Loss (Binary Crossentropy)', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tlla-wp7rDgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9: Confusion Matrix and Metrics**"
      ],
      "metadata": {
        "id": "ypjMs9MYz2lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test_syn, predicted_classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Class 0', 'Class 1'],\n",
        "            yticklabels=['Class 0', 'Class 1'],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Confusion Matrix - Synthetic Data', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test_syn, predicted_classes,\n",
        "                           target_names=['Class 0', 'Class 1'], digits=4))\n",
        "\n",
        "# Calculate metrics manually\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"\\nDetailed Metrics:\")\n",
        "print(f\"True Negatives (TN):  {tn}\")\n",
        "print(f\"False Positives (FP): {fp}\")\n",
        "print(f\"False Negatives (FN): {fn}\")\n",
        "print(f\"True Positives (TP):  {tp}\")\n",
        "print(f\"\\nPrecision: {tp / (tp + fp):.4f}\")\n",
        "print(f\"Recall: {tp / (tp + fn):.4f}\")\n",
        "print(f\"Specificity: {tn / (tn + fp):.4f}\")"
      ],
      "metadata": {
        "id": "wfhZhfwarH4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10: ROC Curve and AUC**"
      ],
      "metadata": {
        "id": "zUcpgTJmz8Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test_syn, predictions)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "         label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.title('ROC Curve - Binary Classification', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"âœ… Area Under Curve (AUC): {roc_auc:.4f}\")\n",
        "print(\"   â€¢ AUC = 1.0: Perfect classifier\")\n",
        "print(\"   â€¢ AUC = 0.5: Random classifier\")\n",
        "print(f\"   â€¢ Our model: {'Excellent' if roc_auc > 0.9 else 'Good' if roc_auc > 0.8 else 'Fair'}\")"
      ],
      "metadata": {
        "id": "_yM1lzolrK8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ¸ PROJECT 2: Iris Flower Binary Classification\n",
        "**Step 11: Load Iris Dataset**"
      ],
      "metadata": {
        "id": "clibyRE1rXaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "# Convert to binary classification: Setosa (0) vs Others (1)\n",
        "y_iris_binary = (y_iris != 0).astype(int)\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "iris_df = pd.DataFrame(X_iris, columns=iris.feature_names)\n",
        "iris_df['species'] = y_iris\n",
        "iris_df['binary_class'] = y_iris_binary\n",
        "iris_df['species_name'] = iris_df['species'].map({0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'})\n",
        "iris_df['binary_name'] = iris_df['binary_class'].map({0: 'Setosa', 1: 'Not Setosa'})\n",
        "\n",
        "print(\"ðŸŒ¸ Iris Dataset Information:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total samples: {len(X_iris)}\")\n",
        "print(f\"Features: {iris.feature_names}\")\n",
        "print(f\"Original classes: {iris.target_names}\")\n",
        "print(f\"\\nBinary classification:\")\n",
        "print(f\"  Class 0 (Setosa): {(y_iris_binary == 0).sum()} samples\")\n",
        "print(f\"  Class 1 (Not Setosa): {(y_iris_binary == 1).sum()} samples\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 samples:\")\n",
        "print(iris_df.head())"
      ],
      "metadata": {
        "id": "5o5LQaCRrTtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 12: Visualize Iris Dataset**"
      ],
      "metadata": {
        "id": "q6Hnj0QO2muy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the dataset\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Pairplot of first two features\n",
        "axes[0, 0].scatter(iris_df[iris_df['binary_class']==0].iloc[:, 0],\n",
        "                   iris_df[iris_df['binary_class']==0].iloc[:, 1],\n",
        "                   c='red', label='Setosa', alpha=0.7, edgecolor='black', s=60)\n",
        "axes[0, 0].scatter(iris_df[iris_df['binary_class']==1].iloc[:, 0],\n",
        "                   iris_df[iris_df['binary_class']==1].iloc[:, 1],\n",
        "                   c='blue', label='Not Setosa', alpha=0.7, edgecolor='black', s=60)\n",
        "axes[0, 0].set_xlabel(iris.feature_names[0], fontweight='bold')\n",
        "axes[0, 0].set_ylabel(iris.feature_names[1], fontweight='bold')\n",
        "axes[0, 0].set_title('Feature 1 vs Feature 2', fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Pairplot of features 3 and 4\n",
        "axes[0, 1].scatter(iris_df[iris_df['binary_class']==0].iloc[:, 2],\n",
        "                   iris_df[iris_df['binary_class']==0].iloc[:, 3],\n",
        "                   c='red', label='Setosa', alpha=0.7, edgecolor='black', s=60)\n",
        "axes[0, 1].scatter(iris_df[iris_df['binary_class']==1].iloc[:, 2],\n",
        "                   iris_df[iris_df['binary_class']==1].iloc[:, 3],\n",
        "                   c='blue', label='Not Setosa', alpha=0.7, edgecolor='black', s=60)\n",
        "axes[0, 1].set_xlabel(iris.feature_names[2], fontweight='bold')\n",
        "axes[0, 1].set_ylabel(iris.feature_names[3], fontweight='bold')\n",
        "axes[0, 1].set_title('Feature 3 vs Feature 4', fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Class distribution\n",
        "axes[1, 0].bar(['Setosa', 'Not Setosa'],\n",
        "               [(y_iris_binary == 0).sum(), (y_iris_binary == 1).sum()],\n",
        "               color=['red', 'blue'], edgecolor='black', linewidth=2)\n",
        "axes[1, 0].set_ylabel('Number of Samples', fontweight='bold')\n",
        "axes[1, 0].set_title('Binary Class Distribution', fontweight='bold')\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Feature correlation heatmap\n",
        "correlation = iris_df.iloc[:, :4].corr()\n",
        "sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "            ax=axes[1, 1], cbar_kws={'label': 'Correlation'})\n",
        "axes[1, 1].set_title('Feature Correlation Matrix', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7RCVTn7prg61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 13: Prepare Iris Data**"
      ],
      "metadata": {
        "id": "3giQpwC620ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris_binary, test_size=0.2, random_state=42, stratify=y_iris_binary\n",
        ")\n",
        "\n",
        "print(\"Data Split:\")\n",
        "print(f\"Training samples: {X_train_iris.shape[0]}\")\n",
        "print(f\"Test samples: {X_test_iris.shape[0]}\")\n",
        "print(f\"Features: {X_train_iris.shape[1]}\")\n",
        "\n",
        "# Standardize features\n",
        "scaler_iris = StandardScaler()\n",
        "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
        "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
        "\n",
        "print(f\"\\nâœ… Data preprocessed and scaled\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hODsrAtkrkF7",
        "outputId": "a3418d99-a01b-435c-dbdf-f419b0f14433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Split:\n",
            "Training samples: 120\n",
            "Test samples: 30\n",
            "Features: 4\n",
            "\n",
            "âœ… Data preprocessed and scaled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 14: Build Iris Classification Model**"
      ],
      "metadata": {
        "id": "sj0qVn_R3D8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model for Iris dataset\n",
        "iris_model = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(4,)),\n",
        "    Dropout(0.2),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "iris_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display architecture\n",
        "print(\"ðŸŒ¸ Iris Binary Classification Model:\")\n",
        "iris_model.summary()"
      ],
      "metadata": {
        "id": "LmeZO67xrsEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 15: Train Iris Model**"
      ],
      "metadata": {
        "id": "Jqiudiru3NXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "print(\"ðŸš€ Training Iris Classification Model...\")\n",
        "\n",
        "history_iris = iris_model.fit(\n",
        "    X_train_iris_scaled, y_train_iris,\n",
        "    epochs=100,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    verbose=0  # Set to 1 to see epoch-by-epoch progress\n",
        ")\n",
        "\n",
        "print(\"âœ… Training completed!\")\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history_iris.history['accuracy'], label='Training', linewidth=2)\n",
        "ax1.plot(history_iris.history['val_accuracy'], label='Validation', linewidth=2)\n",
        "ax1.set_title('Iris Model - Accuracy', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history_iris.history['loss'], label='Training', linewidth=2)\n",
        "ax2.plot(history_iris.history['val_loss'], label='Validation', linewidth=2)\n",
        "ax2.set_title('Iris Model - Loss', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QXz1IrZUru_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 16: Evaluate Iris Model**"
      ],
      "metadata": {
        "id": "GR6KVDGI3RBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "test_loss_iris, test_accuracy_iris = iris_model.evaluate(X_test_iris_scaled, y_test_iris, verbose=0)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸŒ¸ IRIS DATASET - MODEL EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Test Accuracy: {test_accuracy_iris:.4f} ({test_accuracy_iris*100:.2f}%)\")\n",
        "print(f\"Test Loss: {test_loss_iris:.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Predictions\n",
        "predictions_iris = iris_model.predict(X_test_iris_scaled)\n",
        "predicted_classes_iris = (predictions_iris > 0.5).astype(int).flatten()\n",
        "\n",
        "# Confusion matrix\n",
        "cm_iris = confusion_matrix(y_test_iris, predicted_classes_iris)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_iris, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Setosa', 'Not Setosa'],\n",
        "            yticklabels=['Setosa', 'Not Setosa'])\n",
        "plt.title('Confusion Matrix - Iris Binary Classification', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test_iris, predicted_classes_iris,\n",
        "                           target_names=['Setosa', 'Not Setosa'], digits=4))"
      ],
      "metadata": {
        "id": "1K7A8eCur23i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 17: Visualize Decision Boundary (2D)**"
      ],
      "metadata": {
        "id": "181Vz2Dz3Vwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create decision boundary visualization using first 2 features\n",
        "X_train_2d = X_train_iris_scaled[:, :2]\n",
        "X_test_2d = X_test_iris_scaled[:, :2]\n",
        "\n",
        "# Train a simpler model on 2 features for visualization\n",
        "simple_model = Sequential([\n",
        "    Dense(8, activation='relu', input_shape=(2,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "simple_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "simple_model.fit(X_train_2d, y_train_iris, epochs=50, verbose=0)\n",
        "\n",
        "# Create mesh\n",
        "x_min, x_max = X_train_2d[:, 0].min() - 0.5, X_train_2d[:, 0].max() + 0.5\n",
        "y_min, y_max = X_train_2d[:, 1].min() - 0.5, X_train_2d[:, 1].max() + 0.5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Predict on mesh\n",
        "Z = simple_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
        "plt.scatter(X_test_2d[y_test_iris==0, 0], X_test_2d[y_test_iris==0, 1],\n",
        "            c='red', label='Setosa', edgecolor='black', s=100, alpha=0.8)\n",
        "plt.scatter(X_test_2d[y_test_iris==1, 0], X_test_2d[y_test_iris==1, 1],\n",
        "            c='blue', label='Not Setosa', edgecolor='black', s=100, alpha=0.8)\n",
        "plt.xlabel(f'{iris.feature_names[0]} (scaled)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel(f'{iris.feature_names[1]} (scaled)', fontsize=12, fontweight='bold')\n",
        "plt.title('Decision Boundary - First 2 Features', fontsize=14, fontweight='bold')\n",
        "plt.colorbar(label='Prediction Probability')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nv0mfHM3r8Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 18: Compare Both Projects**"
      ],
      "metadata": {
        "id": "PwgevsjY3dqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“Š FINAL COMPARISON: SYNTHETIC vs IRIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_data = {\n",
        "    'Metric': ['Test Accuracy', 'Test Loss', 'Dataset Size', 'Features'],\n",
        "    'Synthetic Data': [f'{test_accuracy*100:.2f}%', f'{test_loss:.4f}',\n",
        "                      f'{len(X_synthetic)}', n_features],\n",
        "    'Iris Dataset': [f'{test_accuracy_iris*100:.2f}%', f'{test_loss_iris:.4f}',\n",
        "                    f'{len(X_iris)}', '4']\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Visual comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "models = ['Synthetic Data', 'Iris Dataset']\n",
        "accuracies = [test_accuracy * 100, test_accuracy_iris * 100]\n",
        "colors = ['#FF6B6B', '#51CF66']\n",
        "\n",
        "bars = ax.bar(models, accuracies, color=colors, width=0.5,\n",
        "              edgecolor='black', linewidth=2)\n",
        "\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{acc:.2f}%',\n",
        "            ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Test Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Binary Classification Performance Comparison',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.set_ylim([90, 102])\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WD1l2PNSr_sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 19: Key Insights and Summary**"
      ],
      "metadata": {
        "id": "q4w93L4g3jMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸŽ“ KEY LEARNINGS: BINARY CLASSIFICATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "1. âœ… SIGMOID ACTIVATION\n",
        "   â€¢ Output range: [0, 1] (probability)\n",
        "   â€¢ Used in output layer for binary classification\n",
        "   â€¢ Threshold: 0.5 (adjustable based on needs)\n",
        "\n",
        "2. âœ… BINARY CROSSENTROPY LOSS\n",
        "   â€¢ Specialized loss for binary classification\n",
        "   â€¢ Penalizes confident wrong predictions more\n",
        "   â€¢ Formula: -[y*log(Å·) + (1-y)*log(1-Å·)]\n",
        "\n",
        "3. âœ… KEY DIFFERENCES FROM MULTI-CLASS\n",
        "   â”‚ Aspect              â”‚ Binary          â”‚ Multi-class       â”‚\n",
        "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "   â”‚ Output neurons      â”‚ 1               â”‚ n (num classes)  â”‚\n",
        "   â”‚ Activation          â”‚ Sigmoid         â”‚ Softmax          â”‚\n",
        "   â”‚ Loss function       â”‚ Binary CE       â”‚ Categorical CE   â”‚\n",
        "   â”‚ Output range        â”‚ [0, 1]          â”‚ [0, 1] per class â”‚\n",
        "\n",
        "4. âœ… EVALUATION METRICS\n",
        "   â€¢ Accuracy: Overall correctness\n",
        "   â€¢ Precision: TP / (TP + FP) - How many predicted positives are correct\n",
        "   â€¢ Recall: TP / (TP + FN) - How many actual positives were found\n",
        "   â€¢ F1-Score: Harmonic mean of precision and recall\n",
        "   â€¢ ROC-AUC: Area under ROC curve (threshold-independent)\n",
        "\n",
        "5. âœ… PRACTICAL TIPS\n",
        "   â€¢ Always scale/normalize features for ANNs\n",
        "   â€¢ Monitor both training and validation metrics\n",
        "   â€¢ Use dropout to prevent overfitting\n",
        "   â€¢ Consider class imbalance (use class_weight if needed)\n",
        "   â€¢ ROC-AUC is more informative than accuracy for imbalanced data\n",
        "\"\"\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "-UPWGZgbsCqs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}